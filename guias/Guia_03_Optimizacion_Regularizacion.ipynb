{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guia 03: Optimizacion y Regularizacion en Deep Learning\n",
    "\n",
    "## Electiva II - Deep Learning | Tecnologico de Antioquia\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo de aprendizaje:** Comprender y aplicar tecnicas de optimizacion y regularizacion para mejorar el rendimiento y evitar el sobreajuste en redes neuronales.\n",
    "\n",
    "**Conceptos nuevos:**\n",
    "- Optimizadores: SGD, Adam, RMSprop, Adagrad\n",
    "- Overfitting vs Underfitting\n",
    "- Dropout\n",
    "- Batch Normalization\n",
    "- Early Stopping\n",
    "- Regularizacion L1 y L2\n",
    "- Learning Rate Scheduling\n",
    "\n",
    "**Prerrequisito:** Guia 02 - Redes Neuronales con Keras\n",
    "\n",
    "**Duracion estimada:** 3 horas\n",
    "\n",
    "> **IMPORTANTE:** Las secciones marcadas con ✍️ requieren tu respuesta escrita. Estas respuestas son parte de tu evaluacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuracion del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Importacion de librerias necesarias\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuracion de estilo para graficas\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ============================================================\n",
    "# Semilla para reproducibilidad\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# Verificar version de TensorFlow y GPU disponible\n",
    "# ============================================================\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"\\nEntorno configurado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Marco Teorico\n",
    "\n",
    "### 2.1 El problema del entrenamiento: ¿por que no basta con construir la red?\n",
    "\n",
    "En la Guia 02 aprendimos a construir redes neuronales multicapa con Keras. Pudimos resolver MNIST con buena precision. Sin embargo, **construir la arquitectura es solo la mitad del trabajo**. La otra mitad, igualmente critica, es **como entrenamos** esa red.\n",
    "\n",
    "Imagina que diseñas un auto de carreras perfecto: motor potente, aerodinamica ideal, llantas de alta gama. Pero si no sabes conducirlo bien, no ganaras la carrera. De manera similar, una red neuronal bien diseñada puede fallar miserablemente si:\n",
    "\n",
    "1. **El optimizador es inadecuado** (como usar la marcha equivocada en el auto)\n",
    "2. **El modelo memoriza los datos de entrenamiento** en lugar de aprender patrones generales (overfitting)\n",
    "3. **El modelo es demasiado simple** para capturar la complejidad del problema (underfitting)\n",
    "4. **No sabemos cuando parar** el entrenamiento\n",
    "\n",
    "En esta guia abordaremos sistematicamente cada uno de estos problemas.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Overfitting vs Underfitting\n",
    "\n",
    "Estos son los dos problemas fundamentales del aprendizaje automatico:\n",
    "\n",
    "#### Underfitting (Subajuste)\n",
    "El modelo es **demasiado simple** para capturar los patrones de los datos. Es como un estudiante que no estudia lo suficiente: no aprende ni lo basico.\n",
    "\n",
    "**Sintomas:**\n",
    "- Accuracy bajo en entrenamiento Y en validacion\n",
    "- El modelo no mejora aunque entrene mas epocas\n",
    "- Las curvas de loss de train y validacion son altas y cercanas entre si\n",
    "\n",
    "**Causas comunes:**\n",
    "- Modelo con muy pocas neuronas/capas\n",
    "- Pocas epocas de entrenamiento\n",
    "- Learning rate demasiado bajo\n",
    "- Features insuficientes\n",
    "\n",
    "#### Overfitting (Sobreajuste)\n",
    "El modelo **memoriza** los datos de entrenamiento en lugar de aprender patrones generalizables. Es como un estudiante que se aprende las respuestas del examen de memoria pero no entiende los conceptos: le ira bien en ese examen especifico, pero fallara en cualquier pregunta nueva.\n",
    "\n",
    "**Sintomas:**\n",
    "- Accuracy MUY alto en entrenamiento pero bajo en validacion\n",
    "- La curva de loss de entrenamiento baja pero la de validacion sube\n",
    "- Gran brecha entre rendimiento en train vs test\n",
    "\n",
    "**Causas comunes:**\n",
    "- Modelo demasiado complejo para la cantidad de datos\n",
    "- Pocos datos de entrenamiento\n",
    "- Entrenar demasiadas epocas\n",
    "- No usar regularizacion\n",
    "\n",
    "#### El punto ideal\n",
    "Lo que buscamos es el **equilibrio** entre underfitting y overfitting: un modelo lo suficientemente complejo para capturar patrones reales, pero no tanto que memorice ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Grafica conceptual: Underfitting vs Overfitting vs Ideal\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Datos de ejemplo con ruido\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 4, 30)\n",
    "y_real = np.sin(x * 1.5)\n",
    "y_ruido = y_real + np.random.normal(0, 0.2, len(x))\n",
    "x_fino = np.linspace(0, 4, 200)\n",
    "\n",
    "# Underfitting: modelo lineal\n",
    "coefs_1 = np.polyfit(x, y_ruido, 1)\n",
    "y_under = np.polyval(coefs_1, x_fino)\n",
    "axes[0].scatter(x, y_ruido, c='steelblue', s=40, alpha=0.7, label='Datos')\n",
    "axes[0].plot(x_fino, y_under, 'r-', linewidth=2, label='Modelo (lineal)')\n",
    "axes[0].plot(x_fino, np.sin(x_fino * 1.5), 'g--', linewidth=1.5, alpha=0.5, label='Patron real')\n",
    "axes[0].set_title('UNDERFITTING\\n(Modelo muy simple)', fontsize=13, fontweight='bold', color='orange')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].set_ylim(-2, 2)\n",
    "\n",
    "# Buen ajuste: polinomio grado 4\n",
    "coefs_4 = np.polyfit(x, y_ruido, 4)\n",
    "y_good = np.polyval(coefs_4, x_fino)\n",
    "axes[1].scatter(x, y_ruido, c='steelblue', s=40, alpha=0.7, label='Datos')\n",
    "axes[1].plot(x_fino, y_good, 'r-', linewidth=2, label='Modelo (grado 4)')\n",
    "axes[1].plot(x_fino, np.sin(x_fino * 1.5), 'g--', linewidth=1.5, alpha=0.5, label='Patron real')\n",
    "axes[1].set_title('BUEN AJUSTE\\n(Modelo adecuado)', fontsize=13, fontweight='bold', color='green')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].set_ylim(-2, 2)\n",
    "\n",
    "# Overfitting: polinomio grado 25\n",
    "coefs_25 = np.polyfit(x, y_ruido, 25)\n",
    "y_over = np.polyval(coefs_25, x_fino)\n",
    "y_over = np.clip(y_over, -2, 2)  # Recortar para visualizar mejor\n",
    "axes[2].scatter(x, y_ruido, c='steelblue', s=40, alpha=0.7, label='Datos')\n",
    "axes[2].plot(x_fino, y_over, 'r-', linewidth=2, label='Modelo (grado 25)')\n",
    "axes[2].plot(x_fino, np.sin(x_fino * 1.5), 'g--', linewidth=1.5, alpha=0.5, label='Patron real')\n",
    "axes[2].set_title('OVERFITTING\\n(Modelo muy complejo)', fontsize=13, fontweight='bold', color='red')\n",
    "axes[2].legend(fontsize=9)\n",
    "axes[2].set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparacion visual de Underfitting, Buen Ajuste y Overfitting', \n",
    "             fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Optimizadores: el arte de descender la montaña\n",
    "\n",
    "El entrenamiento de una red neuronal consiste en encontrar los pesos que **minimizan la funcion de perdida**. Esto se logra mediante **descenso por gradiente** y sus variantes.\n",
    "\n",
    "Imagina que estas en la cima de una montaña cubierta de niebla y debes llegar al punto mas bajo (el valle). No puedes ver el valle, solo puedes sentir la inclinacion del terreno bajo tus pies. Los optimizadores son **diferentes estrategias para bajar la montaña**.\n",
    "\n",
    "---\n",
    "\n",
    "#### SGD (Stochastic Gradient Descent) - El caminante basico\n",
    "\n",
    "La forma mas simple. En cada paso, das un paso en la direccion de mayor pendiente.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla L(\\theta_t)$$\n",
    "\n",
    "Donde:\n",
    "- $\\theta$ son los parametros (pesos) del modelo\n",
    "- $\\eta$ es el learning rate (tamaño del paso)\n",
    "- $\\nabla L$ es el gradiente de la funcion de perdida\n",
    "\n",
    "**Analogia:** Caminas montaña abajo dando pasos del mismo tamaño siempre, sin memoria de pasos anteriores. Si el terreno es muy irregular, puedes avanzar lentamente o quedarte oscilando.\n",
    "\n",
    "**Ventajas:** Simple, funciona bien con learning rate adecuado.  \n",
    "**Desventajas:** Lento, puede quedarse en minimos locales, sensible al learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### SGD con Momentum - El caminante con inercia\n",
    "\n",
    "Agrega \"inercia\" al movimiento, acumulando velocidad en la direccion correcta.\n",
    "\n",
    "$$v_{t+1} = \\beta \\cdot v_t + \\eta \\cdot \\nabla L(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - v_{t+1}$$\n",
    "\n",
    "Donde $\\beta$ es el coeficiente de momentum (tipicamente 0.9).\n",
    "\n",
    "**Analogia:** Como una bola rodando montaña abajo: acumula velocidad en pendientes consistentes y atraviesa pequeños baches sin detenerse. Esto le permite escapar de minimos locales superficiales.\n",
    "\n",
    "**Ventajas:** Mas rapido que SGD basico, escapa minimos locales.  \n",
    "**Desventajas:** Un hiperparametro mas que ajustar ($\\beta$).\n",
    "\n",
    "---\n",
    "\n",
    "#### Adagrad - El caminante adaptativo\n",
    "\n",
    "Adapta el learning rate para cada parametro individualmente. Parametros que se actualizan frecuentemente reciben learning rates mas pequeños.\n",
    "\n",
    "$$\\theta_{t+1, i} = \\theta_{t, i} - \\frac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot \\nabla L(\\theta_{t, i})$$\n",
    "\n",
    "Donde $G_t$ es la suma de los cuadrados de los gradientes pasados.\n",
    "\n",
    "**Analogia:** El caminante ajusta el tamaño de sus pasos segun lo transitado: por donde ya paso muchas veces, da pasos pequeños (ya conoce bien esa zona); por terreno nuevo, da pasos grandes.\n",
    "\n",
    "**Ventajas:** No requiere ajustar el learning rate manualmente.  \n",
    "**Desventajas:** El learning rate puede hacerse tan pequeño que el aprendizaje se detiene.\n",
    "\n",
    "---\n",
    "\n",
    "#### RMSprop - El caminante con memoria selectiva\n",
    "\n",
    "Resuelve el problema de Adagrad usando un promedio movil exponencial de los gradientes al cuadrado.\n",
    "\n",
    "$$E[g^2]_t = \\gamma \\cdot E[g^2]_{t-1} + (1 - \\gamma) \\cdot g_t^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t$$\n",
    "\n",
    "**Analogia:** Similar a Adagrad, pero \"olvida\" gradientes muy antiguos, manteniendo solo la historia reciente. Esto evita que el learning rate se haga cero.\n",
    "\n",
    "**Ventajas:** Buen rendimiento en problemas no estacionarios.  \n",
    "**Desventajas:** Tiene un hiperparametro adicional ($\\gamma$, tipicamente 0.9).\n",
    "\n",
    "---\n",
    "\n",
    "#### Adam (Adaptive Moment Estimation) - El mejor de ambos mundos\n",
    "\n",
    "Combina las ideas de Momentum y RMSprop. Mantiene promedios moviles tanto del gradiente (primer momento) como del gradiente al cuadrado (segundo momento).\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t \\quad \\text{(primer momento)}$$\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\quad \\text{(segundo momento)}$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\quad \\text{(correccion de sesgo)}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t$$\n",
    "\n",
    "**Analogia:** Es el caminante mas experimentado: tiene inercia (momentum) para mantener la direccion correcta, Y adapta el tamaño de sus pasos al terreno (como RMSprop). Ademas, corrige sus estimaciones al inicio cuando tiene poca informacion.\n",
    "\n",
    "**Ventajas:** Funciona bien con valores por defecto, rapido, robusto.  \n",
    "**Desventajas:** Puede no generalizar tan bien como SGD con momentum en algunos casos.\n",
    "\n",
    "> **Valores por defecto de Adam:** $\\eta = 0.001$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-7}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.4 Regularizacion L1 y L2: penalizar pesos grandes\n",
    "\n",
    "La regularizacion agrega un **termino de penalizacion** a la funcion de perdida para evitar que los pesos crezcan demasiado. Pesos grandes significan que el modelo depende demasiado de ciertas entradas, lo cual favorece la memorizacion.\n",
    "\n",
    "#### Regularizacion L2 (Ridge / Weight Decay)\n",
    "\n",
    "Agrega la suma de los cuadrados de los pesos a la funcion de perdida:\n",
    "\n",
    "$$L_{total} = L_{original} + \\lambda \\sum_{i} w_i^2$$\n",
    "\n",
    "**Efecto:** Los pesos se hacen pequeños pero NO cero. Es como un impuesto proporcional: cuanto mas grande el peso, mayor la penalizacion.\n",
    "\n",
    "#### Regularizacion L1 (Lasso)\n",
    "\n",
    "Agrega la suma de los valores absolutos de los pesos:\n",
    "\n",
    "$$L_{total} = L_{original} + \\lambda \\sum_{i} |w_i|$$\n",
    "\n",
    "**Efecto:** Lleva algunos pesos exactamente a cero, creando un modelo \"sparse\" (disperso). Util para seleccion de features.\n",
    "\n",
    "El parametro $\\lambda$ controla la fuerza de la regularizacion:\n",
    "- $\\lambda$ pequeño: poca regularizacion, riesgo de overfitting\n",
    "- $\\lambda$ grande: mucha regularizacion, riesgo de underfitting\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Dropout: apagar neuronas aleatoriamente\n",
    "\n",
    "Dropout es una tecnica de regularizacion donde, durante el entrenamiento, se **desactivan aleatoriamente** un porcentaje de neuronas en cada paso.\n",
    "\n",
    "**Analogia del equipo de trabajo:** Imagina un equipo de 10 personas. Si cada dia 3 personas aleatorias no vienen a trabajar, el equipo no puede depender de ningun miembro individual. Todos deben aprender a hacer de todo, creando un equipo mas robusto y versatil. De manera similar, Dropout obliga a la red a no depender de ninguna neurona individual, distribuyendo el conocimiento.\n",
    "\n",
    "**Detalles importantes:**\n",
    "- Dropout **solo se aplica durante el entrenamiento**, no durante la prediccion\n",
    "- Las salidas se escalan por $\\frac{1}{1-p}$ durante el entrenamiento para compensar las neuronas apagadas\n",
    "- Valores tipicos: 0.2 a 0.5 (20% a 50% de neuronas desactivadas)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Batch Normalization: normalizar activaciones entre capas\n",
    "\n",
    "Batch Normalization normaliza las activaciones de cada capa para que tengan media 0 y desviacion estandar 1, luego aplica una transformacion aprendida:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "\n",
    "Donde $\\mu_B$ y $\\sigma_B$ son la media y desviacion del mini-batch, y $\\gamma$ y $\\beta$ son parametros aprendidos.\n",
    "\n",
    "**Beneficios:**\n",
    "- Permite usar learning rates mas altos\n",
    "- Acelera la convergencia significativamente\n",
    "- Tiene un ligero efecto regularizador\n",
    "- Reduce la dependencia de la inicializacion de pesos\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7 Early Stopping: saber cuando parar\n",
    "\n",
    "Early Stopping monitorea el rendimiento en el conjunto de validacion y **detiene el entrenamiento cuando deja de mejorar**.\n",
    "\n",
    "**Parametros clave:**\n",
    "- **monitor:** metrica a vigilar (tipicamente `val_loss`)\n",
    "- **patience:** numero de epocas sin mejora antes de parar\n",
    "- **restore_best_weights:** restaurar los pesos de la mejor epoca\n",
    "\n",
    "**Analogia:** Es como estudiar para un examen: hay un punto optimo de estudio. Si sigues estudiando demasiado despues de ese punto, puedes confundirte con detalles irrelevantes. Early Stopping es tu alarma que te dice \"ya aprendiste suficiente, para aqui\".\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8 Learning Rate Scheduling\n",
    "\n",
    "La idea es **reducir el learning rate** durante el entrenamiento. Al inicio, un learning rate grande permite avanzar rapido. Luego, un learning rate menor permite ajustes finos.\n",
    "\n",
    "**Estrategias comunes:**\n",
    "- **Step Decay:** Reducir por un factor cada N epocas\n",
    "- **Exponential Decay:** $\\eta_t = \\eta_0 \\cdot e^{-kt}$\n",
    "- **ReduceLROnPlateau:** Reducir cuando la metrica deja de mejorar\n",
    "\n",
    "**Analogia:** Al principio de un viaje corres para cubrir distancia. Cuando te acercas al destino, caminas para no pasarte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Carga y Exploracion de Datos: Fashion-MNIST\n",
    "\n",
    "Fashion-MNIST es un dataset de imagenes de ropa creado por Zalando Research como reemplazo mas desafiante de MNIST. Tiene la misma estructura (28x28 pixeles, escala de grises, 10 clases) pero es significativamente mas dificil.\n",
    "\n",
    "**Las 10 categorias son:**\n",
    "\n",
    "| Etiqueta | Clase |\n",
    "|----------|-------|\n",
    "| 0 | Camiseta/Top |\n",
    "| 1 | Pantalon |\n",
    "| 2 | Sueter |\n",
    "| 3 | Vestido |\n",
    "| 4 | Abrigo |\n",
    "| 5 | Sandalia |\n",
    "| 6 | Camisa |\n",
    "| 7 | Zapatilla |\n",
    "| 8 | Bolso |\n",
    "| 9 | Bota |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cargar Fashion-MNIST\n",
    "# ============================================================\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Nombres de las clases en español\n",
    "nombres_clases = ['Camiseta/Top', 'Pantalon', 'Sueter', 'Vestido', 'Abrigo',\n",
    "                   'Sandalia', 'Camisa', 'Zapatilla', 'Bolso', 'Bota']\n",
    "\n",
    "print(f\"Datos de entrenamiento: {x_train_full.shape}\")\n",
    "print(f\"Datos de test: {x_test.shape}\")\n",
    "print(f\"Rango de pixeles: [{x_train_full.min()}, {x_train_full.max()}]\")\n",
    "print(f\"Clases unicas: {np.unique(y_train_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualizar ejemplos de cada categoria\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    # Encontrar el primer ejemplo de la clase i\n",
    "    idx = np.where(y_train_full == i)[0][0]\n",
    "    ax.imshow(x_train_full[idx], cmap='gray')\n",
    "    ax.set_title(f'{i}: {nombres_clases[i]}', fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Ejemplos de las 10 categorias de Fashion-MNIST', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Distribucion de clases en el dataset\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "valores, conteos = np.unique(y_train_full, return_counts=True)\n",
    "barras = ax.bar([nombres_clases[v] for v in valores], conteos, color=sns.color_palette('Set2', 10))\n",
    "ax.set_title('Distribucion de clases en Fashion-MNIST (entrenamiento)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Cantidad de imagenes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Agregar valores sobre las barras\n",
    "for barra, conteo in zip(barras, conteos):\n",
    "    ax.text(barra.get_x() + barra.get_width()/2., barra.get_height() + 50,\n",
    "            str(conteo), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Preprocesamiento de datos\n",
    "# ============================================================\n",
    "\n",
    "# 1. Normalizar pixeles al rango [0, 1]\n",
    "x_train_full = x_train_full.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 2. Aplanar imagenes de 28x28 a vectores de 784\n",
    "x_train_full_flat = x_train_full.reshape(-1, 784)\n",
    "x_test_flat = x_test.reshape(-1, 784)\n",
    "\n",
    "# 3. Dividir en entrenamiento y validacion (50000 train, 10000 val)\n",
    "x_train = x_train_full_flat[:50000]\n",
    "x_val = x_train_full_flat[50000:]\n",
    "y_train = y_train_full[:50000]\n",
    "y_val = y_train_full[50000:]\n",
    "\n",
    "# 4. One-hot encoding de las etiquetas\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_val_cat = to_categorical(y_val, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Entrenamiento: {x_train.shape}, etiquetas: {y_train_cat.shape}\")\n",
    "print(f\"Validacion: {x_val.shape}, etiquetas: {y_val_cat.shape}\")\n",
    "print(f\"Test: {x_test_flat.shape}, etiquetas: {y_test_cat.shape}\")\n",
    "print(f\"\\nRango de pixeles despues de normalizar: [{x_train.min():.1f}, {x_train.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Funcion auxiliar para graficar curvas de entrenamiento\n",
    "\n",
    "Antes de comenzar los experimentos, definiremos una funcion reutilizable para graficar las curvas de perdida y precision durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Funcion auxiliar para graficar curvas de entrenamiento\n",
    "# ============================================================\n",
    "def graficar_historia(historia, titulo='Curvas de Entrenamiento'):\n",
    "    \"\"\"\n",
    "    Grafica las curvas de loss y accuracy del entrenamiento.\n",
    "    \n",
    "    Parametros:\n",
    "    -----------\n",
    "    historia : keras History object\n",
    "        El objeto retornado por model.fit()\n",
    "    titulo : str\n",
    "        Titulo del grafico\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Curva de Loss\n",
    "    ax1.plot(historia.history['loss'], label='Train Loss', linewidth=2)\n",
    "    ax1.plot(historia.history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n",
    "    ax1.set_title('Perdida (Loss)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoca')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Curva de Accuracy\n",
    "    ax2.plot(historia.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    ax2.plot(historia.history['val_accuracy'], label='Val Accuracy', linewidth=2, linestyle='--')\n",
    "    ax2.set_title('Precision (Accuracy)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoca')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(titulo, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Funcion graficar_historia() definida correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Experimentacion Guiada\n",
    "\n",
    "A continuacion realizaremos 7 experimentos sistematicos para entender el impacto de cada tecnica de optimizacion y regularizacion.\n",
    "\n",
    "---\n",
    "\n",
    "### EXPERIMENTO 1: Provocar Overfitting Intencionalmente\n",
    "\n",
    "**Objetivo:** Construir un modelo demasiado grande y entrenarlo con pocos datos para provocar overfitting claro. Esto nos servira como **linea base** para comparar las tecnicas de regularizacion.\n",
    "\n",
    "**Estrategia:**\n",
    "- Modelo grande: 3 capas ocultas de 512 neuronas cada una\n",
    "- Solo 500 muestras de entrenamiento (muy pocas)\n",
    "- 100 epocas (muchas)\n",
    "- Sin ninguna tecnica de regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 1: Provocar Overfitting\n",
    "# ============================================================\n",
    "\n",
    "# Tomar solo 500 muestras para entrenar (provocar overfitting)\n",
    "x_train_small = x_train[:500]\n",
    "y_train_small_cat = y_train_cat[:500]\n",
    "\n",
    "print(f\"Muestras de entrenamiento: {x_train_small.shape[0]}\")\n",
    "print(f\"Muestras de validacion: {x_val.shape[0]}\")\n",
    "print(f\"Ratio train/val: {x_train_small.shape[0]/x_val.shape[0]:.2f}\")\n",
    "print(\"\\nEste ratio tan bajo casi garantiza overfitting con un modelo grande.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Construir un modelo deliberadamente grande para overfitting\n",
    "# ============================================================\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "modelo_overfit = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(512, activation='relu'),    # Capa oculta 1: 512 neuronas\n",
    "    layers.Dense(512, activation='relu'),    # Capa oculta 2: 512 neuronas\n",
    "    layers.Dense(512, activation='relu'),    # Capa oculta 3: 512 neuronas\n",
    "    layers.Dense(10, activation='softmax')   # Capa de salida: 10 clases\n",
    "])\n",
    "\n",
    "modelo_overfit.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Ver resumen del modelo\n",
    "modelo_overfit.summary()\n",
    "print(f\"\\nTotal de parametros: {modelo_overfit.count_params():,}\")\n",
    "print(f\"Muestras de entrenamiento: {x_train_small.shape[0]}\")\n",
    "print(f\"Ratio parametros/muestras: {modelo_overfit.count_params()/x_train_small.shape[0]:.0f}\")\n",
    "print(\"\\nCon tantos parametros y tan pocos datos, el overfitting es inevitable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Entrenar el modelo overfit por 100 epocas\n",
    "# ============================================================\n",
    "print(\"Entrenando modelo sobreajustado (100 epocas, 500 muestras)...\")\n",
    "print(\"Observa como la brecha entre train y validacion crece con las epocas.\\n\")\n",
    "\n",
    "historia_overfit = modelo_overfit.fit(\n",
    "    x_train_small, y_train_small_cat,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar las curvas de entrenamiento del modelo overfit\n",
    "# ============================================================\n",
    "graficar_historia(historia_overfit, 'Experimento 1: Overfitting Intencional (500 muestras, modelo grande)')\n",
    "\n",
    "# Mostrar metricas finales\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS DEL EXPERIMENTO 1\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train Loss final:  {historia_overfit.history['loss'][-1]:.4f}\")\n",
    "print(f\"Val Loss final:    {historia_overfit.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Train Accuracy:    {historia_overfit.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Val Accuracy:      {historia_overfit.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"\\nBrecha de accuracy: {historia_overfit.history['accuracy'][-1] - historia_overfit.history['val_accuracy'][-1]:.4f}\")\n",
    "print(\"\\nUna brecha grande indica overfitting severo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 1\n",
    "\n",
    "Observa las graficas anteriores y responde:\n",
    "\n",
    "1. ¿En que epoca aproximadamente comienza el overfitting? ¿Como lo identificas en las graficas?\n",
    "2. ¿Cual es la brecha entre el accuracy de entrenamiento y el de validacion al final del entrenamiento?\n",
    "3. ¿Por que crees que el modelo memoriza los datos en lugar de aprender patrones generales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 2: Comparacion de Optimizadores\n",
    "\n",
    "**Objetivo:** Comparar el rendimiento de 5 optimizadores diferentes entrenando el mismo modelo con los mismos datos.\n",
    "\n",
    "**Optimizadores a comparar:**\n",
    "1. SGD con lr=0.01 (sin momentum)\n",
    "2. SGD con lr=0.01 y momentum=0.9\n",
    "3. RMSprop con lr=0.001\n",
    "4. Adam con lr=0.001\n",
    "5. Adagrad con lr=0.01\n",
    "\n",
    "Usaremos el dataset **completo** y un modelo de tamaño moderado para que la comparacion sea justa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Funcion para crear el modelo base (mismo para todos los optimizadores)\n",
    "# ============================================================\n",
    "def crear_modelo_base():\n",
    "    \"\"\"\n",
    "    Crea un modelo con la misma arquitectura para\n",
    "    comparar optimizadores de forma justa.\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(SEED)\n",
    "    modelo = keras.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return modelo\n",
    "\n",
    "print(\"Funcion crear_modelo_base() definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 2: Entrenar con 5 optimizadores diferentes\n",
    "# ============================================================\n",
    "\n",
    "# Definir los optimizadores a comparar\n",
    "optimizadores = {\n",
    "    'SGD (lr=0.01)': keras.optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': keras.optimizers.Adam(learning_rate=0.001),\n",
    "    'Adagrad': keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "}\n",
    "\n",
    "# Diccionario para almacenar las historias de entrenamiento\n",
    "historias_opt = {}\n",
    "\n",
    "EPOCAS_OPT = 30  # Suficiente para ver las diferencias\n",
    "\n",
    "for nombre, optimizador in optimizadores.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Entrenando con: {nombre}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Crear un modelo nuevo con la misma arquitectura\n",
    "    modelo = crear_modelo_base()\n",
    "    modelo.compile(\n",
    "        optimizer=optimizador,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Entrenar\n",
    "    historia = modelo.fit(\n",
    "        x_train, y_train_cat,\n",
    "        epochs=EPOCAS_OPT,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_val, y_val_cat),\n",
    "        verbose=0  # Silencioso para no saturar la salida\n",
    "    )\n",
    "    \n",
    "    historias_opt[nombre] = historia\n",
    "    \n",
    "    # Mostrar resultado final\n",
    "    val_acc = historia.history['val_accuracy'][-1]\n",
    "    val_loss = historia.history['val_loss'][-1]\n",
    "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTodos los optimizadores entrenados exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar comparacion de optimizadores: Loss\n",
    "# ============================================================\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colores = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for i, (nombre, historia) in enumerate(historias_opt.items()):\n",
    "    ax1.plot(historia.history['val_loss'], label=nombre, linewidth=2, color=colores[i])\n",
    "    ax2.plot(historia.history['val_accuracy'], label=nombre, linewidth=2, color=colores[i])\n",
    "\n",
    "ax1.set_title('Comparacion de Loss en Validacion', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Epoca')\n",
    "ax1.set_ylabel('Validation Loss')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_title('Comparacion de Accuracy en Validacion', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Epoca')\n",
    "ax2.set_ylabel('Validation Accuracy')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 2: Comparacion de Optimizadores', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Tabla comparativa de optimizadores\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA COMPARATIVA DE OPTIMIZADORES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Optimizador':<20} {'Val Acc Final':>14} {'Val Loss Final':>15} {'Mejor Val Acc':>14} {'Epoca Mejor':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for nombre, historia in historias_opt.items():\n",
    "    val_acc_final = historia.history['val_accuracy'][-1]\n",
    "    val_loss_final = historia.history['val_loss'][-1]\n",
    "    mejor_val_acc = max(historia.history['val_accuracy'])\n",
    "    epoca_mejor = np.argmax(historia.history['val_accuracy']) + 1\n",
    "    \n",
    "    print(f\"{nombre:<20} {val_acc_final:>14.4f} {val_loss_final:>15.4f} {mejor_val_acc:>14.4f} {epoca_mejor:>12}\")\n",
    "\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 2\n",
    "\n",
    "Analiza las graficas y la tabla comparativa:\n",
    "\n",
    "1. ¿Cual optimizador convergio mas rapido (alcanzo buen accuracy en menos epocas)?\n",
    "2. ¿Cual obtuvo el mejor resultado final en accuracy?\n",
    "3. ¿Siempre coincide el que converge mas rapido con el que obtiene mejor resultado final? ¿Por que?\n",
    "4. ¿Por que Adam es el optimizador mas popular en la practica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 3: Regularizacion L1 y L2\n",
    "\n",
    "**Objetivo:** Aplicar regularizacion L2 con diferentes intensidades al modelo sobreajustado del Experimento 1 y observar como reduce el overfitting.\n",
    "\n",
    "Vamos a retomar la configuracion del Experimento 1 (modelo grande, solo 500 muestras) y probar regularizacion L2 con valores:\n",
    "- Sin regularizacion (linea base)\n",
    "- L2 = 0.001 (suave)\n",
    "- L2 = 0.01 (moderada)\n",
    "- L2 = 0.1 (fuerte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 3: Regularizacion L2 con diferentes valores\n",
    "# ============================================================\n",
    "\n",
    "valores_l2 = {\n",
    "    'Sin regularizacion': None,\n",
    "    'L2 = 0.001': 0.001,\n",
    "    'L2 = 0.01': 0.01,\n",
    "    'L2 = 0.1': 0.1\n",
    "}\n",
    "\n",
    "historias_l2 = {}\n",
    "\n",
    "for nombre, valor_l2 in valores_l2.items():\n",
    "    print(f\"\\nEntrenando: {nombre}...\")\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Crear modelo con o sin regularizacion L2\n",
    "    if valor_l2 is not None:\n",
    "        reg = regularizers.l2(valor_l2)\n",
    "    else:\n",
    "        reg = None\n",
    "    \n",
    "    modelo_l2 = keras.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    modelo_l2.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    historia = modelo_l2.fit(\n",
    "        x_train_small, y_train_small_cat,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_val, y_val_cat),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    historias_l2[nombre] = historia\n",
    "    print(f\"  Train Acc: {historia.history['accuracy'][-1]:.4f} | Val Acc: {historia.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nExperimento 3 completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar comparacion de regularizacion L2\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colores_l2 = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, (nombre, historia) in enumerate(historias_l2.items()):\n",
    "    # Loss de validacion\n",
    "    axes[0].plot(historia.history['val_loss'], label=nombre, linewidth=2, color=colores_l2[i])\n",
    "    # Accuracy de validacion\n",
    "    axes[1].plot(historia.history['val_accuracy'], label=nombre, linewidth=2, color=colores_l2[i])\n",
    "\n",
    "axes[0].set_title('Validation Loss con diferentes L2', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Val Loss')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation Accuracy con diferentes L2', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Val Accuracy')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 3: Efecto de la Regularizacion L2', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLA COMPARATIVA - REGULARIZACION L2\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Configuracion':<22} {'Train Acc':>10} {'Val Acc':>10} {'Brecha':>10} {'Mejor Val Acc':>14}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for nombre, historia in historias_l2.items():\n",
    "    train_acc = historia.history['accuracy'][-1]\n",
    "    val_acc = historia.history['val_accuracy'][-1]\n",
    "    brecha = train_acc - val_acc\n",
    "    mejor = max(historia.history['val_accuracy'])\n",
    "    print(f\"{nombre:<22} {train_acc:>10.4f} {val_acc:>10.4f} {brecha:>10.4f} {mejor:>14.4f}\")\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 3\n",
    "\n",
    "1. ¿Que efecto tuvo la regularizacion L2 sobre el overfitting? Compara la brecha train-val entre los diferentes valores.\n",
    "2. ¿Que pasa si el valor de regularizacion es demasiado alto (L2=0.1)? ¿Mejora o empeora el modelo?\n",
    "3. ¿Cual valor de L2 te parece el mejor compromiso? ¿Por que?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 4: Dropout\n",
    "\n",
    "**Objetivo:** Aplicar Dropout con diferentes tasas al modelo sobreajustado y observar su efecto.\n",
    "\n",
    "Vamos a comparar 4 configuraciones:\n",
    "- Sin Dropout (base sobreajustada)\n",
    "- Dropout de 0.2 (20% neuronas desactivadas)\n",
    "- Dropout de 0.5 (50% neuronas desactivadas)\n",
    "- Dropout de 0.8 (80% neuronas desactivadas - excesivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 4: Dropout con diferentes tasas\n",
    "# ============================================================\n",
    "\n",
    "tasas_dropout = {\n",
    "    'Sin Dropout': 0.0,\n",
    "    'Dropout 0.2': 0.2,\n",
    "    'Dropout 0.5': 0.5,\n",
    "    'Dropout 0.8': 0.8\n",
    "}\n",
    "\n",
    "historias_dropout = {}\n",
    "\n",
    "for nombre, tasa in tasas_dropout.items():\n",
    "    print(f\"\\nEntrenando: {nombre}...\")\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Construir modelo con Dropout\n",
    "    capas = [layers.Input(shape=(784,))]\n",
    "    for _ in range(3):\n",
    "        capas.append(layers.Dense(512, activation='relu'))\n",
    "        if tasa > 0:\n",
    "            capas.append(layers.Dropout(tasa))\n",
    "    capas.append(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    modelo_drop = keras.Sequential(capas)\n",
    "    \n",
    "    modelo_drop.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    historia = modelo_drop.fit(\n",
    "        x_train_small, y_train_small_cat,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_val, y_val_cat),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    historias_dropout[nombre] = historia\n",
    "    print(f\"  Train Acc: {historia.history['accuracy'][-1]:.4f} | Val Acc: {historia.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nExperimento 4 completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar comparacion de Dropout\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colores_drop = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, (nombre, historia) in enumerate(historias_dropout.items()):\n",
    "    axes[0].plot(historia.history['val_loss'], label=nombre, linewidth=2, color=colores_drop[i])\n",
    "    axes[1].plot(historia.history['val_accuracy'], label=nombre, linewidth=2, color=colores_drop[i])\n",
    "\n",
    "axes[0].set_title('Validation Loss con diferentes Dropout', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Val Loss')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation Accuracy con diferentes Dropout', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Val Accuracy')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 4: Efecto del Dropout', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLA COMPARATIVA - DROPOUT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Configuracion':<18} {'Train Acc':>10} {'Val Acc':>10} {'Brecha':>10} {'Mejor Val Acc':>14}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for nombre, historia in historias_dropout.items():\n",
    "    train_acc = historia.history['accuracy'][-1]\n",
    "    val_acc = historia.history['val_accuracy'][-1]\n",
    "    brecha = train_acc - val_acc\n",
    "    mejor = max(historia.history['val_accuracy'])\n",
    "    print(f\"{nombre:<18} {train_acc:>10.4f} {val_acc:>10.4f} {brecha:>10.4f} {mejor:>14.4f}\")\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 4\n",
    "\n",
    "1. ¿Cual tasa de Dropout dio el mejor resultado en validacion?\n",
    "2. ¿Que pasa con Dropout demasiado alto (0.8)? Describe el comportamiento que observas.\n",
    "3. ¿Por que Dropout se desactiva automaticamente durante la prediccion (inferencia)? ¿Que pasaria si no se desactivara?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 5: Batch Normalization\n",
    "\n",
    "**Objetivo:** Comparar un modelo con y sin Batch Normalization para observar su efecto en la velocidad de convergencia y el accuracy final.\n",
    "\n",
    "Usaremos el dataset completo para este experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 5: Batch Normalization\n",
    "# ============================================================\n",
    "\n",
    "# --- Modelo SIN Batch Normalization ---\n",
    "print(\"Entrenando modelo SIN Batch Normalization...\")\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "modelo_sin_bn = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_sin_bn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historia_sin_bn = modelo_sin_bn.fit(\n",
    "    x_train, y_train_cat,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    verbose=0\n",
    ")\n",
    "print(f\"  Val Accuracy final: {historia_sin_bn.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# --- Modelo CON Batch Normalization ---\n",
    "print(\"\\nEntrenando modelo CON Batch Normalization...\")\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "modelo_con_bn = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),       # BN despues de la capa densa\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),       # BN despues de la capa densa\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),       # BN despues de la capa densa\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_con_bn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historia_con_bn = modelo_con_bn.fit(\n",
    "    x_train, y_train_cat,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    verbose=0\n",
    ")\n",
    "print(f\"  Val Accuracy final: {historia_con_bn.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nExperimento 5 completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar comparacion Batch Normalization\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(historia_sin_bn.history['val_loss'], label='Sin BN', linewidth=2, color='#e74c3c')\n",
    "axes[0].plot(historia_con_bn.history['val_loss'], label='Con BN', linewidth=2, color='#2ecc71')\n",
    "axes[0].set_title('Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Val Loss')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(historia_sin_bn.history['val_accuracy'], label='Sin BN', linewidth=2, color='#e74c3c')\n",
    "axes[1].plot(historia_con_bn.history['val_accuracy'], label='Con BN', linewidth=2, color='#2ecc71')\n",
    "axes[1].set_title('Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Val Accuracy')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 5: Efecto de Batch Normalization', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparacion numerica\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACION BATCH NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Modelo':<20} {'Val Acc Epoca 5':>15} {'Val Acc Epoca 15':>16} {'Val Acc Final':>14}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Sin BN':<20} {historia_sin_bn.history['val_accuracy'][4]:>15.4f} {historia_sin_bn.history['val_accuracy'][14]:>16.4f} {historia_sin_bn.history['val_accuracy'][-1]:>14.4f}\")\n",
    "print(f\"{'Con BN':<20} {historia_con_bn.history['val_accuracy'][4]:>15.4f} {historia_con_bn.history['val_accuracy'][14]:>16.4f} {historia_con_bn.history['val_accuracy'][-1]:>14.4f}\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 5\n",
    "\n",
    "1. ¿Como afecto Batch Normalization a la velocidad de convergencia? Compara el accuracy en las primeras 5 epocas.\n",
    "2. ¿Y al accuracy final despues de 30 epocas?\n",
    "3. ¿En que situaciones crees que Batch Normalization seria mas beneficioso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 6: Early Stopping\n",
    "\n",
    "**Objetivo:** Comparar un modelo entrenado por 200 epocas sin restriccion contra uno con Early Stopping que se detiene automaticamente cuando deja de mejorar.\n",
    "\n",
    "Usaremos el dataset completo y un modelo mediano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 6A: Entrenar SIN Early Stopping (200 epocas)\n",
    "# ============================================================\n",
    "print(\"Entrenando modelo SIN Early Stopping (200 epocas)...\")\n",
    "print(\"Esto puede tomar unos minutos.\\n\")\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "modelo_sin_es = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_sin_es.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historia_sin_es = modelo_sin_es.fit(\n",
    "    x_train, y_train_cat,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss_sin_es, test_acc_sin_es = modelo_sin_es.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
    "print(f\"Modelo SIN Early Stopping:\")\n",
    "print(f\"  Epocas entrenadas: 200\")\n",
    "print(f\"  Test Accuracy: {test_acc_sin_es:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 6B: Entrenar CON Early Stopping\n",
    "# ============================================================\n",
    "print(\"Entrenando modelo CON Early Stopping (max 200 epocas, patience=10)...\\n\")\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "modelo_con_es = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_con_es.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Definir Early Stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',           # Monitorear la perdida de validacion\n",
    "    patience=10,                  # Esperar 10 epocas sin mejora antes de parar\n",
    "    restore_best_weights=True,    # Restaurar los pesos de la mejor epoca\n",
    "    verbose=1                     # Mostrar cuando se detiene\n",
    ")\n",
    "\n",
    "historia_con_es = modelo_con_es.fit(\n",
    "    x_train, y_train_cat,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss_con_es, test_acc_con_es = modelo_con_es.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
    "epocas_entrenadas = len(historia_con_es.history['loss'])\n",
    "print(f\"\\nModelo CON Early Stopping:\")\n",
    "print(f\"  Epocas entrenadas: {epocas_entrenadas}\")\n",
    "print(f\"  Test Accuracy: {test_acc_con_es:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar comparacion Early Stopping\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(historia_sin_es.history['val_loss'], label='Sin Early Stopping (200 ep)', \n",
    "             linewidth=2, color='#e74c3c', alpha=0.8)\n",
    "axes[0].plot(historia_con_es.history['val_loss'], label=f'Con Early Stopping ({epocas_entrenadas} ep)', \n",
    "             linewidth=2, color='#2ecc71')\n",
    "axes[0].axvline(x=epocas_entrenadas-1, color='#2ecc71', linestyle=':', alpha=0.7, label='Punto de parada')\n",
    "axes[0].set_title('Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Val Loss')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(historia_sin_es.history['val_accuracy'], label='Sin Early Stopping (200 ep)', \n",
    "             linewidth=2, color='#e74c3c', alpha=0.8)\n",
    "axes[1].plot(historia_con_es.history['val_accuracy'], label=f'Con Early Stopping ({epocas_entrenadas} ep)', \n",
    "             linewidth=2, color='#2ecc71')\n",
    "axes[1].axvline(x=epocas_entrenadas-1, color='#2ecc71', linestyle=':', alpha=0.7, label='Punto de parada')\n",
    "axes[1].set_title('Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Val Accuracy')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 6: Efecto de Early Stopping', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparacion final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACION EARLY STOPPING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Modelo':<30} {'Epocas':>8} {'Test Acc':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Sin Early Stopping':<30} {'200':>8} {test_acc_sin_es:>10.4f}\")\n",
    "print(f\"{'Con Early Stopping (p=10)':<30} {epocas_entrenadas:>8} {test_acc_con_es:>10.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nEpocas ahorradas: {200 - epocas_entrenadas}\")\n",
    "print(f\"Diferencia en Test Accuracy: {test_acc_con_es - test_acc_sin_es:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 6\n",
    "\n",
    "1. ¿En que epoca se detuvo el entrenamiento con Early Stopping?\n",
    "2. ¿El modelo con Early Stopping obtuvo mejor accuracy en test que el que entreno las 200 epocas? ¿Por que?\n",
    "3. ¿Que significa el parametro `patience=10`? ¿Que pasaria si usaras `patience=1`? ¿Y `patience=50`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EXPERIMENTO 7: Modelo Final - Combinando Todo\n",
    "\n",
    "**Objetivo:** Construir un modelo que combine las mejores tecnicas aprendidas y compararlo con el modelo base sobreajustado del Experimento 1.\n",
    "\n",
    "**Combinacion a usar:**\n",
    "- Optimizador Adam\n",
    "- Dropout (0.3)\n",
    "- Batch Normalization\n",
    "- Early Stopping (patience=15)\n",
    "- Dataset completo\n",
    "- Arquitectura moderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 7: Modelo final combinando todas las tecnicas\n",
    "# ============================================================\n",
    "print(\"Construyendo el modelo final con todas las tecnicas combinadas...\\n\")\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "modelo_final = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    \n",
    "    # Capa 1: Dense + BN + Dropout\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Capa 2: Dense + BN + Dropout\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Capa 3: Dense + BN + Dropout\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Capa de salida\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_final.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "modelo_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Definir callbacks para el modelo final\n",
    "# ============================================================\n",
    "\n",
    "# Early Stopping con patience generoso\n",
    "early_stop_final = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau: reducir learning rate cuando el modelo se estanca\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,           # Reducir el lr a la mitad\n",
    "    patience=5,           # Esperar 5 epocas sin mejora\n",
    "    min_lr=1e-6,          # Learning rate minimo\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Callbacks configurados:\")\n",
    "print(\"  - EarlyStopping: patience=15, restaura mejores pesos\")\n",
    "print(\"  - ReduceLROnPlateau: reduce lr x0.5 cada 5 epocas sin mejora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Entrenar el modelo final\n",
    "# ============================================================\n",
    "print(\"Entrenando modelo final (max 200 epocas con Early Stopping)...\\n\")\n",
    "\n",
    "historia_final = modelo_final.fit(\n",
    "    x_train, y_train_cat,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    callbacks=[early_stop_final, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "epocas_final = len(historia_final.history['loss'])\n",
    "print(f\"\\nEntrenamiento finalizado en {epocas_final} epocas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Graficar curvas del modelo final\n",
    "# ============================================================\n",
    "graficar_historia(historia_final, 'Experimento 7: Modelo Final (Adam + Dropout + BN + Early Stopping)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluar modelo final en test y comparar con modelo base\n",
    "# ============================================================\n",
    "\n",
    "# Evaluar modelo final en test\n",
    "test_loss_final, test_acc_final = modelo_final.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
    "\n",
    "# Evaluar modelo base (overfit) en test\n",
    "test_loss_base, test_acc_base = modelo_overfit.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACION FINAL: Modelo Base vs Modelo Optimizado\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Modelo':<35} {'Datos Train':>12} {'Test Acc':>10} {'Epocas':>8}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Exp1: Base sobreajustado':<35} {'500':>12} {test_acc_base:>10.4f} {'100':>8}\")\n",
    "print(f\"{'Exp7: Final optimizado':<35} {'50,000':>12} {test_acc_final:>10.4f} {epocas_final:>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "mejora = test_acc_final - test_acc_base\n",
    "mejora_pct = (mejora / test_acc_base) * 100\n",
    "print(f\"\\nMejora absoluta en Test Accuracy: {mejora:+.4f}\")\n",
    "print(f\"Mejora porcentual: {mejora_pct:+.1f}%\")\n",
    "print(f\"\\nEl modelo final es significativamente mejor gracias a:\")\n",
    "print(f\"  1. Mas datos de entrenamiento (50,000 vs 500)\")\n",
    "print(f\"  2. Regularizacion con Dropout\")\n",
    "print(f\"  3. Batch Normalization para convergencia estable\")\n",
    "print(f\"  4. Early Stopping para evitar sobreajuste\")\n",
    "print(f\"  5. Learning Rate Scheduling para ajuste fino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta del Experimento 7\n",
    "\n",
    "1. ¿Cuanto mejoro el modelo final respecto al modelo base del Experimento 1 (en terminos de accuracy en test)?\n",
    "2. ¿Cuales de las tecnicas aplicadas consideras las mas impactantes? ¿Por que?\n",
    "3. ¿Crees que se podria mejorar aun mas el modelo? ¿Como?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sintesis y Reflexion Final\n",
    "\n",
    "Has completado 7 experimentos donde aplicaste sistematicamente tecnicas de optimizacion y regularizacion. Ahora es momento de integrar todo lo aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta de Sintesis 1\n",
    "\n",
    "Crea una tabla resumen con las tecnicas vistas en esta guia. La tabla debe incluir:\n",
    "- Nombre de la tecnica\n",
    "- Que problema resuelve\n",
    "- Cuando usarla\n",
    "- Hiperparametros clave\n",
    "\n",
    "Tecnicas a incluir: SGD con Momentum, Adam, Regularizacion L2, Dropout, Batch Normalization, Early Stopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "| Tecnica | Problema que resuelve | Cuando usarla | Hiperparametros clave |\n",
    "|---------|----------------------|---------------|----------------------|\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "*Completa la tabla...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta de Sintesis 2\n",
    "\n",
    "Si recibes un modelo que tiene **overfitting severo** (train accuracy 99%, validation accuracy 60%), ¿cuales serian los primeros 3 pasos que tomarias para solucionarlo? Explica cada paso y por que lo priorizas en ese orden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Pregunta de Sintesis 3\n",
    "\n",
    "¿Crees que es posible **eliminar completamente** el overfitting? ¿Por que si o por que no? Justifica tu respuesta con lo aprendido en esta guia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Tu respuesta:\n",
    "\n",
    "*Escribe aqui tu respuesta...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Reto Extra\n",
    "\n",
    "### Desafio: Consigue el Mayor Accuracy Posible en Fashion-MNIST\n",
    "\n",
    "**Objetivo:** Usando TODAS las tecnicas aprendidas en esta guia, construye el mejor modelo posible para Fashion-MNIST.\n",
    "\n",
    "**Reglas:**\n",
    "1. Solo puedes usar capas Dense (no convolucionales, esas seran en la Guia 05)\n",
    "2. Debes probar al menos **3 combinaciones diferentes** de tecnicas\n",
    "3. Documenta cada intento: arquitectura, hiperparametros, resultado\n",
    "4. **Objetivo minimo: 90% accuracy en test**\n",
    "\n",
    "**Sugerencias de cosas que puedes variar:**\n",
    "- Numero de capas y neuronas por capa\n",
    "- Tasa de Dropout\n",
    "- Learning rate y optimizador\n",
    "- Usar o no Batch Normalization\n",
    "- Valores de regularizacion L2\n",
    "- Patience del Early Stopping\n",
    "- Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETO: Intento 1\n",
    "# ============================================================\n",
    "# Describe tu arquitectura y estrategia aqui:\n",
    "# - Arquitectura: ...\n",
    "# - Optimizador: ...\n",
    "# - Regularizacion: ...\n",
    "# - Batch size: ...\n",
    "# ============================================================\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Tu modelo aqui\n",
    "modelo_reto_1 = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    # Agrega tus capas aqui...\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_reto_1.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "historia_reto_1 = modelo_reto_1.fit(\n",
    "    x_train, y_train_cat,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val_cat),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "_, acc_reto_1 = modelo_reto_1.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
    "print(f\"Intento 1 - Test Accuracy: {acc_reto_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETO: Intento 2\n",
    "# ============================================================\n",
    "# Modifica la arquitectura y/o hiperparametros:\n",
    "# - Arquitectura: ...\n",
    "# - Optimizador: ...\n",
    "# - Regularizacion: ...\n",
    "# ============================================================\n",
    "\n",
    "# Escribe tu segundo intento aqui...\n",
    "# (Puedes copiar y modificar el codigo del Intento 1)\n",
    "\n",
    "print(\"Escribe tu segundo intento aqui...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETO: Intento 3\n",
    "# ============================================================\n",
    "# Modifica la arquitectura y/o hiperparametros:\n",
    "# - Arquitectura: ...\n",
    "# - Optimizador: ...\n",
    "# - Regularizacion: ...\n",
    "# ============================================================\n",
    "\n",
    "# Escribe tu tercer intento aqui...\n",
    "# (Puedes copiar y modificar el codigo del Intento 1)\n",
    "\n",
    "print(\"Escribe tu tercer intento aqui...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETO: Tabla resumen de resultados\n",
    "# ============================================================\n",
    "# Completa esta tabla con tus resultados\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TABLA DE RESULTADOS DEL RETO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Intento':<12} {'Arquitectura':<25} {'Test Accuracy':>14} {'Objetivo':>10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Intento 1':<12} {'512-256-128 + BN + Drop':<25} {acc_reto_1:>14.4f} {'>=0.90':>10}\")\n",
    "print(f\"{'Intento 2':<12} {'(completar)':<25} {'(completar)':>14} {'>=0.90':>10}\")\n",
    "print(f\"{'Intento 3':<12} {'(completar)':<25} {'(completar)':>14} {'>=0.90':>10}\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\n¿Alcanzaste el 90%? Si no, ¿que mas podrias intentar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Referencias\n",
    "\n",
    "### Articulos fundamentales\n",
    "- **Adam optimizer:** Kingma, D. P., & Ba, J. (2015). *Adam: A Method for Stochastic Optimization*. ICLR.\n",
    "- **Dropout:** Srivastava, N., et al. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*. JMLR.\n",
    "- **Batch Normalization:** Ioffe, S., & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*. ICML.\n",
    "- **Fashion-MNIST:** Xiao, H., Rasul, K., & Vollgraf, R. (2017). *Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms*.\n",
    "\n",
    "### Documentacion oficial\n",
    "- TensorFlow / Keras: [https://www.tensorflow.org/api_docs](https://www.tensorflow.org/api_docs)\n",
    "- Keras Optimizers: [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)\n",
    "- Keras Regularizers: [https://keras.io/api/layers/regularizers/](https://keras.io/api/layers/regularizers/)\n",
    "- Keras Callbacks: [https://keras.io/api/callbacks/](https://keras.io/api/callbacks/)\n",
    "\n",
    "### Recursos adicionales recomendados\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Capitulo 7: Regularization.\n",
    "- Chollet, F. (2021). *Deep Learning with Python*, 2nd Edition. Manning Publications.\n",
    "\n",
    "---\n",
    "\n",
    "**Electiva II - Deep Learning | Tecnologico de Antioquia | 2026-1**\n",
    "\n",
    "*Proxima guia: Guia 04 - Metricas de Evaluacion y Matrices de Confusion*"
   ]
  }
 ]
}
